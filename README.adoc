// = Kokkidio Readme
// :author: Lennart Steffen
// :email: Lennart.Steffen@wahyd.tu-berlin.de
:source-highlighter: highlight.js
:toc:
:toc-placement!:

:Eigen: https://eigen.tuxfamily.org/[Eigen]
:Kokkos: https://kokkos.org/[Kokkos]
:GPLv3: https://www.gnu.org/licenses/gpl-3.0.en.html[GPLv3]
:wahyd: https://www.wahyd.tu-berlin.de/
:maplink: https://eigen.tuxfamily.org/dox/classEigen_1_1Map.html

:viewmap: link:./include/Kokkidio/ViewMap.hpp[ViewMap]
:dualviewmap: link:./include/Kokkidio/DualViewMap.hpp[DualViewMap]
:parallelrange: link:./include/Kokkidio/ParallelRange.hpp[ParallelRange]


image::./media/Kokkidio_Logo.svg[]

_Kokkidio_ is a header-only template library 
designed to provide interoperability between the linear algebra library {Eigen} 
and the performance portability framework {Kokkos}. 
Its aim is to allow the easy-to-read, succinct source code of {Eigen} 
to be compiled to fast machine code on all the platforms supported by {Kokkos}.  

toc::[]


== Overview

_Kokkidio_ 
is built on top of both {kokkos} and {eigen}, and
consists of three interlocking elements:

* An iteration range class (`ParallelRange`), 
which allows threads to efficiently access their assigned elements, 
* parallel dispatch functions (`parallel_for` and `parallel_reduce`) 
compatible with functors taking `ParallelRange`, and
* the <<_data_structures, data structures>> `ViewMap` and `DualViewMap`, 
which combine an `Eigen::Map` and a `Kokkos::View`.

It allows you to write code like this:

.`ViewMap` construction, copying data, and example computation (SAXPY) with _Kokkidio_
[%collapsible,id=kokkidio_ex]
====
(taken from link:./src/examples/axpy.cpp[example/axpy.cpp])
[,cpp]
----
using namespace Kokkidio;
float a {0.5};
int size {10};

using FloatArray = DualViewMap<Eigen::ArrayXf>;
/* You may have an existing Eigen object */
Eigen::ArrayXf x_existing {size};
/* No need to replace it. To make it accessible inside a Kokkos functor
 * (and thus also on GPUs), you can simply wrap it in a (Dual)ViewMap: */
FloatArray x {x_existing};

/* Of course, you can also construct (Dual)ViewMaps from sizes */
FloatArray y {size}, z {size};

/* You can use Kokkos functions on (Dual)ViewMaps, because their members
 * "MapView::view", and
 * "DualViewMap::view_<target>()" 
 * return a Kokkos::View */
Kokkos::deep_copy( y.view_host(), 123 );

/* Likewise, you can use Eigen functions on (Dual)ViewMaps, as their members
 * "MapView::map", and
 * "DualViewMap::map_<target>()"
 * return an Eigen::Map.
 * Outside of a parallel dispatch, only the host side is accessible. */
x.map_host().setRandom();
y.map_host().setRandom();

/* Copying data between host and compute target is simple: */
x.copyToTarget(); // if the compute target is the host, this does nothing
y.copyToTarget();

/* This is how a parallel computation on the target is performed: */
parallel_for( size, KOKKOS_LAMBDA(ParallelRange<> rng){
	rng(z) = a * rng(x) + rng(y);
});
/* After the computation, you may copy the results back to the host */
z.copyToHost();
----
====

== Why Kokkos?

{Kokkos} is a portability and parallelism framework, 
which allows users to write code that compiles and runs 
on a wide range of hardware, especially both CPUs and GPUs.
It provides data structures
(https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/View.html[`Kokkos::View`]),
as well as functions for 
https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/ParallelDispatch.html[parallel dispatch],
and
// It 
may be viewed as a common abstraction 
for various programming models and backends, e.g.
OpenMP, including target offloading, OpenACC, CUDA, HIP, and SYCL.
Here's an example:

.SAXPY with Kokkos
[%collapsible,id=kokkos_ex]
====
[,cpp]
----
float a {0.5};
std::size_t dim1 {10};
/* for more details, see
 * https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/View.html#constructing-a-view
 */
using View = Kokkos::View<float*, Kokkos::DefaultExecutionSpace>;
View x {dim1}, y {dim1}, z {dim1};
/* fill arrays in some way,
 * e.g. using deep_copy or within a parallel dispatch ... */

/* and now do the computation in parallel */
Kokkos::parallel_for( dim1, KOKKOS_LAMBDA(std::size_t i){
	z(i) = a * x(i) + y(i);
});
----
====

While modern compilers already allow targeting multiple CPU architectures,
due to ISAs functioning as a reliable standard (x86(-64), ARM, ...),
GPU architectures and programming models are much less intercompatible.
If you wrote your code using CUDA, 
but want to run it on an Intel PonteVecchio GPU, 
you will have to rewrite large parts of it.

Using {Kokkos} for new projects allows you to avoid that type of effort.
For existing projects, it allows a step-by-step migration, 
because backend-specific code will still compile 
when using Kokkos on top of that backend.

// GPU architectures don't just vary between vendors, 
// but often also between product generations and product lines of a single vendor.
// // e.g. GCN being superseded by RDNA and CDNA, 
// // or <NVIDIA>


// Therefore, expressing a programming task 
// through Kokkos' data structures and parallel dispatch functions
// allows it to be run on basically any hardware

== Why Eigen?

{Eigen} is a linear algebra library, 
which allows writing any such operations in a readable, succinct manner,
while compiling to highly optimised machine code.
Instead of implementing matrix, vector, or coefficient-wise operations 
using loops, the 'Eigen way' is to operate on whole data structures 
using their overloaded operators or member functions. Here's an example:

// .Dot product in pure C++
// [%collapsible,id=eigen_ex]
// ====
// [,cpp]
// ----
// std::size_t size {10};
// std::vector<double> a {size}, b {size};
// /* fill vectors in some way ... */
// /* then loop over them and track the sum */
// double sum {0};
// for (std::size_t i=0; i<size; ++i){
// 	sum += a * b;
// }
// ----
// ====

// .Dot product in Eigen
// [%collapsible]
// ====
// [,cpp]
// ----
// Eigen::Index size {10};
// Eigen::VectorXd a {size}, b {size};
// /* fill vectors in some way ... */
// double sum = a.dot(b);
// ----
// ====


.SAXPY in pure C++
[%collapsible,id=eigen_ex]
====
[,cpp]
----
std::size_t size {10};
double a {0.5};
std::vector<double> x {size}, y {size}, z {size};
/* fill vectors in some way ... */
/* then loop over them and perform the computation element-wise */
for (std::size_t i=0; i<size; ++i){
	z[i] = a * x[i] + y[i];
}
----
====

.SAXPY in Eigen
[%collapsible]
====
[,cpp]
----
Eigen::Index size {10};
double a {0.5};
Eigen::VectorXd x {size}, y {size}, z {size};
/* fill vectors in some way, e.g. using member func setRandom() ... */
/* The computation is expressed with the whole object: */
z = a * x + y;
----
====

In addition to the benefits of letting a well-tested library handle common operations,
Eigen also performs extensive optimisations on the underlying loops.
This notably includes explicit vectorisation for all common CPU vector extensions, 
such as the SSE and AVX families on x86, ARM NEON, and others.
// mention expression templates?

// Since version 3.4, Eigen functions can be called from CUDA/HIP kernels,
// and its compatibility with SYCL is in a usable state since at least early 2024
// and under active development.
// However, neither its allocators for dynamically-sized data structures, 
// nor SIMD parallelism (like vectorisation on CPUs) 
// are currently available on GPUs.
// It does provide the {maplink}[`Eigen::Map`]
// class, though, 
// with which other data structures can be treated like an Eigen object,
// if they expose a data pointer and their memory layout and stride(s) are known.

== And why Kokkidio?

There exist a number of problems when trying to use Eigen's functionality on GPUs 
(e.g. with Kokkos),
which are detailed below -- and which _Kokkidio_ aims to address.

=== The problems

. *Data structures* +
Since version 3.4, Eigen functions can be called from CUDA/HIP kernels,
and its compatibility with SYCL is in a usable state since at least early 2024
and under active development.
However, only fixed-size objects can be created on GPUs -- 
dynamically-sized data structures are only available on and from the host.
There is the {maplink}[`Eigen::Map`] class, though, 
with which other data structures can be treated like an Eigen object,
if they expose a data pointer and their memory layout and stride(s) are known.

. *Loop abstraction and parallelism 1* +
In the <<kokkos_ex,Kokkos example>> above, you can see 
the functor passed to Kokkos' `parallel_for` 
(the `KOKKOS_LAMBDA` in that case) 
containing the instructions for an *individual loop iteration*.
By contrast, the *loop is fully abstracted* in the <<eigen_ex,Eigen example>>.
There is no _user-facing_ loop iteration variable in an Eigen operation 
that could be used to parallelise operations.
+
Of course, that loop still exists, deep within the Eigen library.
While it could potentially be parallelised there, 
// sweeping changes to Eigen's interface would be necessary as well 
// to make this practical.
the interface of Eigen is not set up to facilitate anything like this.
Therefore, sweeping changes to the whole library would be necessary, 
with all the compatibility and testing issues that brings.
// Parallel execution would have to be conditional, 
// as a kernel dispatch would not make sense for every operation,
// and to prevent accidental nesting.

. *Parallelism 2: Chunk sizes, vectorisation, and readability* +
// Furthermore, when 
When
parallelising an Eigen operation as a library user,
the ideal strategy for assigning work to threads 
is rather different between CPUs and GPUs.
On a GPU, the large number of relatively weak cores 
leads to many small units of work performing better.
// a large number of small units of work perform better, 
// while 
By contrast, 
on a CPU, fewer, larger, and contiguous chunks are preferable,
to allow for vectorised operations. 
+
// Let's use a (slightly) more involved example, 
// where an individual thread's operation is still done with Eigen:
// Here's an example with Eigen, where columns of matrices are (dot-) multiplied:
Here's an example to illustrate this difference.
We're not using SAXPY, but instead multiply matrix columns, 
so that individual threads still use Eigen functionality.
+
.Multiply matrix columns
[%collapsible, id=par_issue_eigen]
====
(taken from link:./src/examples/dot.cpp[examples/dot.cpp])
[,cpp]
----
int nRows {4}, nCols {1000};
Eigen::MatrixXd a {nRows, nCols}, b;
b.resizeLike(a);
/* fill matrices in some way ... */
double result; // let's sum up the results to not need another array
/* One could do a nested loop and manually implement the dot product.
 * We skip that here, because for that you wouldn't use Eigen */

/******************************************************************************/
/* OPTION 1, better on GPUs */
/******************************************************************************/
/* Distribute individual column-multiplications, 
 * as one might do on a GPU, if nCols >> nRows */
result = 0;
for (int i=0; i<nCols; ++i){
	result += a.col(i).transpose() * b.col(i);
	/* this is equivalent: */
	// result += a.col(i).dot( b.col(i) );
}

/******************************************************************************/
/* OPTION 2, better on CPUs */
/******************************************************************************/
/* Distribute blocks of the matrices to threads and let Eigen
 * handle the loop over columns, as may be preferable on a CPU.
 * This can be a lot faster, as it allows Eigen to vectorise the operation. */
result = 0;
int nCores {4}; // just for illustration
int nColsPerCore {nCols / nCores}; // not handling remainders

for (int i=0; i<nCores; ++i){
	int firstCol {i * nColsPerCore};
	result += (
		a.middleCols(firstCol, nColsPerCore).transpose() * 
		b.middleCols(firstCol, nColsPerCore)
	).trace(); // trace = sum of the diagonal
}
----
====
Not only do the loop bodies differ quite significantly,
but Eigen's advantage of short and readable source code 
is also somewhat diminished, when parallelising code in this fashion.


=== Approaching solutions

_Kokkidio_ solves the first issue of Eigen's data structures 
not being compatible with GPU programming models by 
wrapping a `Kokkos::View` in a class called `ViewMap`.
It provides access to the contained `Kokkos::View`, 
and additionally uses Eigen's {maplink}[`Map`] feature 
to allow the contained data to be treated like an `Eigen` object --
even on GPUs. More detail is provided in the section <<_data_structures>>.

To address the issues regarding parallelism, vectorisation, and readability,
_Kokkidio_ provides 
an iteration range class 
called <<_parrange, `ParallelRange`>>
and <<_parfor, parallel dispatch functions>> compatible with it.
These are detailed in the next section.


== Parallel dispatch
[id=_pardisp]


// an iteration range class, 
// combined with parallel dispatch functions which use that class.
// The class is called `ParallelRange` 
// and its behaviour is specialised depending on the execution target (CPU/GPU).
// When applying a `ParallelRange` (i.e., its `operator()`) to a `ViewMap`,
// the return object represents the data of that `ViewMap` 
// associated with the calling thread: 
// an individual element or column on a GPU,
// and a segment or block on a CPU.
// See its <<_pardisp, dedicated section>> for more details.
// This arrangement makes reading and writing parallel code much easier,
// while also providing performance benefits on CPUs:
// There, it allows for proper vectorisation, 
// thus speeding up many operations significantly.
// On a GPU, it constitutes a zero-overhead abstraction instead.

// The parallel dispatch functions

=== `ParallelRange`
[id=_parrange]

The class template `ParallelRange` fulfils two functions:
Firstly, 
given some total number or range of items to be processed,
it contains the index or index range associated with a thread,
and secondly, when applied to an Eigen object or `(Dual)ViewMap`,
returns the data at that index or index range as an `Eigen::Block` expression.

Its template parameter `target` can take either of 
the two values of the `Target` enumeration, 
// which can be either 
`host` (CPU) or `device` (e.g., GPU):

* When `target==device`, then `ParallelRange` stores a single index. 
Applying it to an Eigen object or `(Dual)ViewMap` 
returns either a single element, if the object is one-dimensional,
or a column expression, if the object is two-dimensional.
By default, 
https://eigen.tuxfamily.org/dox/group__TopicStorageOrders.html[Eigen objects are column-major],
which is the reason behind this choice.

* When `target==host`, then `ParallelRange` stores 
a starting index and number of elements.
Applying it to an Eigen object or `(Dual)ViewMap`
then returns a contiguous block of elements, 
using `Eigen::segment` on 1D objects, and `Eigen::middleCols` on 2D objects.

(Ranges of) rows instead of columns are also available, 
but require the explicit use of a member function (`ParallelRange::rowRange`),
rather than `ParallelRange::operator()`.



==== Synopsis
[id=_parrange_syn]

.Expand synopsis of ParallelRange
[%collapsible]
====
[,cpp]
----

template<Target _target = DefaultTarget>
class ParallelRange : public EigenRange<_target> {
public:
	static constexpr Target target {_target};
	using Base = EigenRange<target>;
	static constexpr bool
		isDevice {target == Target::device},
		isHost   {target == Target::host};
	using MemberType = std::conditional_t<isHost, IndexRange<Index>, int>;
	using ChunkType = EigenRange<target>;
	using ChunkInfoType = ChunkInfo<target>;

private:
	MemberType m_rng;
	ChunkInfoType m_chunks;
public:
	KOKKOS_FUNCTION ParallelRange() = default;

	/* ParallelRange can be instantiated with:
	 * - an integer, 
	 * - a Kokkidio::IndexRange, or
	 * - a Kokkos::RangePolicy. 
	 */
	template<typename Policy>
	KOKKOS_FUNCTION ParallelRange( const Policy& );

/* inherited from EigenRange: */
	KOKKOS_FUNCTION const MemberType& get() const;
	KOKKOS_FUNCTION       MemberType& get();

	KOKKOS_FUNCTION IndexRange<Index> asIndexRange() const;

	template<typename EigenObj>
	KOKKOS_FUNCTION Eigen::Block<...> colRange( EigenObj&& obj ) const;

	template<typename EigenObj>
	KOKKOS_FUNCTION Eigen::Block<...> rowRange( EigenObj&& obj ) const;

	template<typename EigenObj>
	KOKKOS_FUNCTION Eigen::Block<...> range( EigenObj&& obj ) const;

	/* effectively the same as range(...) */
	template<typename EigenObj>
	KOKKOS_FUNCTION Eigen::Block<...> operator() ( EigenObj&& obj ) const;

/* specific to ParallelRange */
	template<typename Func>
	KOKKOS_FUNCTION void for_each( Func&& func ) const;

	template<typename Func>
	KOKKOS_FUNCTION void for_each_chunk(Func&& func) const;

	KOKKOS_FUNCTION ChunkType make_chunk(Index i) const;
	KOKKOS_FUNCTION const ChunkInfo<target>& chunkInfo() const;
	KOKKOS_FUNCTION inline constexpr Index   chunkSize() const;
	KOKKOS_FUNCTION inline constexpr Index   nChunks  () const;
	KOKKOS_FUNCTION void setChunks(Index chunkSizeMax = chunk::defaultSize);
};


/* if you wish to call another function taking Eigen objects,
 * and wish to apply a range to each of the arguments, you can write
 * apply_range(someFunc, range, someFunc_arg1, someFunc_arg2, ...); */
template<Func, Target t, typename ... Ts>
void apply_range(Func&&, const ParallelRange<t>&, Ts&& ... args);

----
====


=== Parallel dispatch functions
[id=_parfor]

_Kokkidio_ provides drop-in replacements for Kokkos' parallel dispatch functions:

* `parallel_for`, for general tasks, and
* `parallel_reduce`, for reductions.

The main difference to their Kokkos equivalents is, 
that they allow passing a functor which takes a `ParallelRange` as its 
(first) argument, e.g.:

[,cpp]
----
parallel_for(someSizeOrPolicy, KOKKOS_LAMBDA(ParallelRange<target> rng){
	/* do something with rng ... */
});
----

On `device` (e.g., GPU), this chains to `Kokkos::parallel_[for|reduce]`, 
and constructs a `ParallelRange<device>` from a single element index.
On `host` (CPU), this calls a _Kokkidio_-specific function 
emulating OpenMP-logic for distributing work items evenly to threads.
The index range of work items consists of a start index and a number of items,
and is expressed as the <<_indexrange, `IndexRange` class>>.
From this, a `ParallelRange<host>` is created, which, 
when applied to an Eigen or `(Dual)ViewMap` object,
returns a contiguous `Eigen::Block` of data, corresponding to the index range.

If a functor is provided that does not take a `ParallelRange` as its parameter,
_Kokkidio_'s parallel dispatch functions simply forward to their Kokkos equivalent.

==== Examples

== Data structures

=== `ViewMap`

The core of the `ViewMap` class (see link:./include/Kokkidio/ViewMap.hpp[file])
are the two member functions `map()` and `view()`,
which return an `Eigen::Map`, and a `Kokkos::View` respectively, 
and thus allow it to be used in either library's functions.

`ViewMap` takes two template parameters:

. `EigenType`: The `Eigen` class to be used as the map type, 
e.g. `Eigen::MatrixXd` or `Eigen::Array3i`. 
The return type of `map()` behaves the same way as this type. 
Only dense types are currently supported. 
. A `Target` enumeration value, which can be either `host` or `device`. 
This parameter is optional. 
Its default value matches `Kokkos::DefaultExecutionSpace`.

`ViewMap` can be instantiated either using an existing `Eigen` object, 
or using the same size parameters as you would for the `Eigen` type. 
Here's what happens when you create a `ViewMap`:

. With an existing `Eigen` object: 

.. Instantiation on `Target::host`:
No allocation is performed. 
An unmanaged `Kokkos::View` is created, 
using the existing object's data pointer and sizes.

.. Instantiation on `Target::device`:
the `Eigen` object's sizes are used to create a matching managed `Kokkos::View` 
on the device.

. With size parameters: 
A managed `Kokkos::View` is created using these sizes on `Target`.
The same size parameters are allowed as for the respective `Eigen` type.
This means, creating vector types (1D) requires only a single size parameter,
and fixed size types can be created without them.

In all of the above cases, the data pointers of `view()` and `map()` 
contain the same address. 
Furthermore, when instantiating a `ViewMap` with 
a non-const, owning `Eigen` object (i.e. not itself an `Eigen::Map`),
a non-owning pointer to the object is stored 
to allow resizing both the `Kokkos::View` and the `Eigen` object 
via `ViewMap::resize()`.

==== Examples

The examples below are taken from
link:./src/examples/ViewMap.cpp[examples/ViewMap.cpp].

.Expand ViewMap examples
[%collapsible]
====
[,cpp]
----
using namespace Kokkidio;
int nRows {10}, nCols {20};

/* existing Eigen object */
Eigen::ArrayXXd eigenArray {nRows, nCols};

/* Create ViewMap using a constructor or factory function.
 * Deduces Eigen type, and uses default target */
ViewMap mv1 {eigenArray};
auto mv2 = viewMap(eigenArray);

/* Create ViewMap using factory function for specific target,
 * while deducing Eigen type */
auto mv3 = viewMap<Target::host>(eigenArray);

/* Create ViewMap using size parameters. 
 * ArrayXXd is dynamically sized in both dimensions, 
 * so two parameters are required */
ViewMap<Eigen::ArrayXXd> mv4 {nRows, nCols};

/* ArrayXd is a column vector, so only rows are required */
ViewMap<Eigen::ArrayXd> mv5 {nRows};

/* Array3d is a fixed size type, so no parameters are required */
ViewMap<Eigen::Array3d> mv6;

/* set values on host, using Eigen's assignment operator on ViewMap::map() */
mv1.map() = 1;

/* set values on target, using Kokkos::deep_copy with ViewMap::view() */
Kokkos::deep_copy(mv2.view(), 2);

/* set values on target with parallel dispatch: */
/* with Kokkidio::ParallelRange */
parallel_for( mv3.cols(), KOKKOS_LAMBDA(ParallelRange<> rng){
	rng(mv3) = 3;
});

/* or just an integer, using the standard Kokkos-style */
parallel_for( mv4.size(), KOKKOS_LAMBDA(int i){
	mv4.data()[i] = 4;
});
----
====

==== Synopsis

.Expand synopsis of ViewMap
[%collapsible]
====
[,cpp]
----

template<typename _EigenType, Target targetArg = DefaultTarget>
class ViewMap {
public:
	static constexpr Target target { ExecutionTarget<targetArg> };
	using EigenType_host = _EigenType;
	/* EigenType_host and EigenType_target may differ in const-ness */
	using EigenType_target = std::conditional_t<target == Target::host,
		EigenType_host,
		std::remove_const_t<EigenType_host>
	>;

	using ThisType = ViewMap<EigenType_target, target>;

	using Scalar     = typename EigenType_target::Scalar;
	using MapType    = Eigen::Map<EigenType_host>;
	/* only types with a continuous memory layout are currently supported */
	static_assert( is_contiguous<EigenType_target>() );

	/* Translations of "target" into Kokkos spaces */
	using MemorySpace    = Kokkidio::MemorySpace   <target>;
	using ExecutionSpace = Kokkidio::ExecutionSpace<target>;
	/* The Kokkos::View data type is either fully dynamic or fully fixed-size,
	 * i.e. Scalar** or Scalar[nRows][nCols],
	 * and always uses LayoutLeft */
	using ViewType   = Kokkos::View<..., Kokkos::LayoutLeft, MemorySpace>;
	using HostMirror = typename ViewType::HostMirror;

public:

	/* constructors */
	ViewMap(); // default, allocation only for fixed size types
	ViewMap(Index size); // 1D types
	ViewMap(Index rows, Index cols); // 2D types
	ViewMap( _EigenType& hostObj ); // existing Eigen objects

	/* "resize" and constructors can only be called from host */
	void resize(Index rows, Index cols);

	/* get some info about type and status */
	KOKKOS_FUNCTION constexpr bool isManaged() const;
	KOKKOS_FUNCTION bool isAlloc() const;

	/* data pointer */
	KOKKOS_FUNCTION Scalar* data();
	KOKKOS_FUNCTION const Scalar* data() const;

	/* get Eigen::Map */
	KOKKOS_FUNCTION MapType map() const;

	/* and Kokkos::View */
	KOKKOS_FUNCTION ViewType view() const;

	/* sizes */
	KOKKOS_FUNCTION Index rows() const;
	KOKKOS_FUNCTION Index cols() const;
	KOKKOS_FUNCTION Index size() const;
};

/* detection */
template<typename T>
inline constexpr bool is_ViewMap_v = ...;


/* factory functions */

/* specify target, deduce EigenType */
template<Target target = DefaultTarget, typename EigenType>
ViewMap<EigenType, target> viewMap( EigenType& eigenObj );

/* specify EigenType, optionally specify target, fixed size */
template<typename EigenType, Target target = DefaultTarget>
ViewMap<EigenType, target> viewMap();

/* specify EigenType, optionally specify target, 1D */
template<typename EigenType, Target target = DefaultTarget>
ViewMap<EigenType, target> viewMap(Index vectorSize);

/* specify EigenType, optionally specify target, 2D */
template<typename EigenType, Target target = DefaultTarget>
ViewMap<EigenType, target> viewMap(Index rows, Index cols);
----
====

=== `DualViewMap`

`DualViewMap` (see link:./include/Kokkidio/DualViewMap.hpp[file])
is designed to facilitate easy data exchange between `host` 
and the compute `Target`. 
To this end, it provides the member functions
`copyToTarget()` 
and 
`copyToHost()`.


It takes the same template parameters as <<_viewmap,`ViewMap`>>, 
i.e. an `Eigen` type, and a `Target` value.
While a `ViewMap` only exists on _either_ `host` or `device`, 
`DualViewMap` always consists of _two_ ``ViewMap``s, 
of which one is located on `host`, 
and the other on the specified `Target`. 
If the `Target` is also `host`, then the two views are identical,
and `copyTo...()` operations are correspondingly skipped.

To access the ``ViewMap``s, it provides the member functions
`get_host()`
and
`get_target()`,
as well as shortcuts to their ``map()``/``view()`` member functions 
in the form of
``map_host()``/``map_target()`` and ``view_host()``/``view_target()``.

Similar to <<_viewmap,`ViewMap`>>, it also allows to `resize()` its data,
and does so on both `host` and the specified `Target`.

==== Examples

The examples below are taken from
link:./src/examples/DualViewMap.cpp[examples/DualViewMap.cpp].

.Expand DualViewMap examples
[%collapsible]
====
[,cpp]
----
using namespace Kokkidio;
int nRows {10}, nCols {20};

/* existing Eigen object */
Eigen::ArrayXXd eigenArray {nRows, nCols};
/* By default, when initialising with an Eigen object,
 * the object's data is copied to the target. 
 * This behaviour be changed with an optional parameter: DontCopyToTarget */
DualViewMap d1 {eigenArray};
auto d2 = dualViewMap(eigenArray, DontCopyToTarget);
/* Otherwise, a DualViewMap can be created in exactly the same ways as a 
 * ViewMap, so please refer to ViewMap.cpp for more examples. */

/* with DualViewMap, you can set your values on host, 
 * then copy them to the target: */
d2.map_host() = 123;
d2.copyToTarget();

auto print = [&](std::string_view descriptor){
	std::cout
		<< "d2, values on host, " << descriptor << ":\n"
		<< d2.map_host() << '\n';
};
print("before");

/* Now you can do some computations on the target, 
 * then copy the values back */
parallel_for(d2.cols(), KOKKOS_LAMBDA(ParallelRange<> rng){
	rng(d2) += 1;
});
d2.copyToHost();

print("after");
----
====

==== Synopsis

.Expand synopsis of DualViewMap
[%collapsible]
====
[,cpp]
----

template<typename _EigenType, Target targetArg = DefaultTarget>
class DualViewMap {
public:
	static constexpr Target target { ExecutionTarget<targetArg> };
	using EigenType_host = _EigenType;

	using ThisType = DualViewMap<EigenType_host, target>;
	using ViewMap_host   = ViewMap<EigenType_host, Target::host>;
	using ViewMap_target = ViewMap<EigenType_host, target>;
	using EigenType_target = typename ViewMap_target::EigenType_target;
	using Scalar = typename ViewMap_target::Scalar;

	using ViewType_host   = typename ViewMap_host  ::ViewType;
	using ViewType_target = typename ViewMap_target::ViewType;
	using ExecutionSpace_target = typename ViewMap_target::ExecutionSpace;

	using MapType_host   = typename ViewMap_host  ::MapType;
	using MapType_target = typename ViewMap_target::MapType;

public:

	/* constructors */
	DualViewMap(); // default, allocation only for fixed size types
	DualViewMap(Index size); // 1D types
	DualViewMap(Index rows, Index cols); // 2D types
	DualViewMap(
		EigenType_host& hostObj,
		DualViewCopyOnInit copyToTarget = CopyToTarget
	);  // existing Eigen objects


	/* "assign", "resize" and constructors can only be called from host */
	void assign( EigenType_host& hostObj );
	void resize(Index rows, Index cols);

	/* get some info about type and status */
	KOKKOS_FUNCTION bool isAlloc_host() const;
	KOKKOS_FUNCTION bool isAlloc_target() const;

	/* get ViewMaps */
	KOKKOS_FUNCTION ViewMap_host   get_host  () const;
	KOKKOS_FUNCTION ViewMap_target get_target() const;

	template<Target _target>
	KOKKOS_FUNCTION auto get() const
		-> std::conditional<_target == target, ViewMap_target, ViewMap_host>;

	/* get Kokkos::Views */
	KOKKOS_FUNCTION ViewType_host   view_host  () const;
	KOKKOS_FUNCTION ViewType_target view_target() const;

	template<Target _target>
	KOKKOS_FUNCTION auto view() const
		-> std::conditional<_target == target, ViewType_target, ViewType_host>;

	/* shortcut to view_target */
	KOKKOS_FUNCTION ViewType_target view() const;

	/* get Eigen::Maps */
	KOKKOS_FUNCTION MapType_host   map_host  () const;
	KOKKOS_FUNCTION MapType_target map_target() const;

	template<Target _target>
	KOKKOS_FUNCTION auto map() const
		-> std::conditional<_target == target, MapType_target, MapType_host>;

	/* shortcut to map_target */
	KOKKOS_FUNCTION MapType_target map() const;

	/* sizes */
	KOKKOS_FUNCTION Index rows() const;
	KOKKOS_FUNCTION Index cols() const;
	KOKKOS_FUNCTION Index size() const;

	/* copy */
	void copyToTarget(bool async = false);
	void copyToHost(bool async = false);
};

/* detection */
template<typename T>
inline constexpr bool is_DualViewMap_v = ...;


/* factory functions */

/* specify target, deduce EigenType */
template<Target target = DefaultTarget, typename EigenType>
DualViewMap<EigenType, target> dualViewMap(
	EigenType& eigenObj,
	DualViewCopyOnInit copyToTarget = CopyToTarget
);

/* specify EigenType, optionally specify target, fixed size */
template<typename EigenType, Target target = DefaultTarget>
DualViewMap<EigenType, target> dualViewMap();

/* specify EigenType, optionally specify target, 1D */
template<typename EigenType, Target target = DefaultTarget>
DualViewMap<EigenType, target> dualViewMap(Index vectorSize);

/* specify EigenType, optionally specify target, 2D */
template<typename EigenType, Target target = DefaultTarget>
DualViewMap<EigenType, target> dualViewMap(Index rows, Index cols);
----
====

== Other

=== `IndexRange`
[id=_indexrange]

asdf

== Notes

=== CMake integration

The multi-backend nature of Kokkos and, by extension, _Kokkidio_, 
makes the process of configuring and building them rather involved, 
so we included a build script to help with this.
// The first step to building _Kokkidio_ is to download and build Kokkos and Eigen.


==== Building Kokkos for _Kokkidio_
To achieve CPU parallelism with _Kokkidio_, 
Kokkos should always be built with the OpenMP backend enabled, i.e.
`KOKKOS_ENABLE_OPENMP=ON`.

=== CPU-Vectorisation with CUDA backend

By default, Eigen disables vectorisation, when `\\__CUDACC__` is defined,
i.e., when `nvcc` is used as the compiler for non-`.cu`-files.
The single-source approach of Kokkos aims to 
eliminate the need for backend-specific files,
thus combining Eigen and Kokkos lets this issue emerge.
// so this issue must arise when combining Eigen and Kokkos, like _Kokkidio_ does.

The fix _Kokkidio_ uses, is to separate CPU and non-CPU translation units,
and then defining `EIGEN_NO_CUDA` for the CPU unit.
For convenience, the CMake function `set_is_cpu` is provided for this purpose.
Here's how to do this in practice:

* Put your source code into some file, 
and don't specify it as a source file in CMake. 
We recommend the file ending `.in` for this. 
Let's call it `source.in`

* Create two additional files, one each for host and device compilation, 
respectively. Let's call these `source_host.cpp` and `source_device.cpp`.
Add these files to your CMake target, and add the line
+
[,cmake]
----
set_is_cpu(source_host.cpp)
----

* `#include` the source file (`source.in`) in both of these files.

* optionally, you can write your functions as templates of the 
`Target` parameter of all _Kokkidio_ classes, 
and explicitly instantiate them in those files.


Here's a full example:

.`source.in`
[%collapsible]
====
[,cpp]
----
#include <Kokkidio.hpp>

template<Target target>
float getSum( const ViewMap<ArrayXf, target>& vm ){
	float sum_global;
	auto func = KOKKOS_LAMBDA(ParallelRange<target> rng, float& sum_thread){
		sum_thread += rng(vm).sum();
	};
	parallel_reduce( vm.size(), func, redux::sum(sum_global) );
	return sum_global;
}

/* explicit instantiation */
template float getSum<MY_SOURCE_TARGET>( const ViewMap<ArrayXf, MY_SOURCE_TARGET>& );
#undef MY_SOURCE_TARGET
----
====

.`source_host.cpp`
[%collapsible]
====
[,cpp]
----
#define MY_SOURCE_TARGET Target::host
#include "source.in"
----
====

.`source_device.cpp`
[%collapsible]
====
[,cpp]
----
#define MY_SOURCE_TARGET Target::device
#include "source.in"
----
====

.`CMakeLists.txt`
[%collapsible]
====
[,cmake]
----
add_library(myTarget
	source_host.cpp
	source_device.cpp
)

set_is_cpu(source_host.cpp)

kokkidio_configure_target(myTarget)
----
====

=== Name and Logo

The name _Kokkidio_ is based on the assumptions that 

. {Kokkos} refers to the Greek *Κόκκος* (engl.: *grain*, though possibly a play on *kernel*), and that 
. {Eigen} refers to eigenvalues and eigenvectors.

The latter are _ιδιοτιμή_ (idiotimí) and _ιδιοδιάνυσμα_ (idiodiánysma) in Greek, 
from which the prefix _ιδιο_ (idio) was taken
(engl.: _same_, though it could also be from _ίδιος_ = own, or self, 
which is the meaning of _eigen_ in German). 
_κοκκίδιο_ (kokkídio) could be seen as a https://en.wikipedia.org/wiki/Portmanteau[portmanteau] of _Kokkos_ and _idio_, 
but is in fact the Greek word for _granule_, so not far off _Kokkos_ itself.

The logo is a stretched/sheared map of a recolouration of the https://kokkos.org/img/kokkos-logo.png[Kokkos logo], 
with the eigenvectors of that mapping drawn as arrows.

== Licence

_Kokkidio_ is maintained by the
Chair of Water Resources Management and Modelling of Hydrosystems of the
Technische Universität Berlin,
or *wahyd* for short ({wahyd}[Link]).
It is distributed under a {gplv3} (link:./LICENCE[Licence text]).
Licence types for the libraries used in _Kokkidio_
are listed in the link:./LICENCE.README[LICENCE.README] file.
